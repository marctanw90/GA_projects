{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a45f5a7",
   "metadata": {},
   "source": [
    "# Project 3 - Web APIs and NLP Classification\n",
    "\n",
    "## Part 1: Web Scraping & Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b99e9f",
   "metadata": {},
   "source": [
    "For project 3, your goal is two-fold:\n",
    "\n",
    "* Using Pushshift's API, you'll collect posts from two subreddits of your choosing.\n",
    "* You'll then use NLP to train a classifier on which subreddit a given post came from. This is a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb18a5",
   "metadata": {},
   "source": [
    "Requirements\n",
    "* Gather and prepare your data using the requests library.\n",
    "* Create and compare two models. One of these must be a Random Forest classifier, however the other can be a classifier of your choosing: logistic regression, KNN, SVM, etc.\n",
    "* A Jupyter Notebook with your analysis for a peer audience of data scientists.\n",
    "* An executive summary of your results.\n",
    "* A short presentation outlining your process and findings for a semi-technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29138c",
   "metadata": {},
   "source": [
    "For Project 3 the evaluation categories are as follows:\n",
    "The Data Science Process\n",
    "\n",
    "* Problem Statement\n",
    "* Data Collection\n",
    "* Data Cleaning & EDA\n",
    "* Preprocessing & Modeling\n",
    "* Evaluation and Conceptual Understanding\n",
    "* Conclusion and Recommendations\n",
    "* Organization and Professionalism\n",
    "\n",
    "Organization\n",
    "* Visualizations\n",
    "* Python Syntax and Control Flow\n",
    "* Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bab01e",
   "metadata": {},
   "source": [
    "### Model to classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f679c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16594392",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'subreddit' : 'EtherMining',\n",
    "    'size': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e297d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f938b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b73c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d23b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387bd23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['subreddit', 'selftext', 'title']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets work on collecting 10,000 post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d70af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters for collecting more submissions\n",
    "\n",
    "def parameters(df, subreddit):\n",
    "    \n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'before': df.loc[(df.shape[0] - 1), 'created_utc']\n",
    "    }\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def function to collect more submissions from subreddit\n",
    "def get_posts(params):\n",
    "    \n",
    "    #url for searching subreddit with Pushshift.io\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    \n",
    "    #scrape submissions data from reddit into json format\n",
    "    res = requests.get(url, params=params)\n",
    "    data = res.json()\n",
    "    \n",
    "    #return data in pandas dataframe format\n",
    "    df = pd.DataFrame(data['data'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ab414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scrape 100 more submissions for a total of 199x, to obtain 20,000 submissions in total\n",
    "\n",
    "for i in trange(1):\n",
    "    \n",
    "    try:\n",
    "        param = parameters(df_em, 'EtherMining')\n",
    "        df_em = pd.concat([df_em, get_posts(param)], ignore_index=True)\n",
    "    \n",
    "    except:\n",
    "        #notifies us if there is an error during scraping\n",
    "        print(f\"Error occurred while scraping\")\n",
    "        \n",
    "    #1 seconds interval per requests to prevent server overload    \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2581f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "file = '../data/EtherMining.csv'\n",
    "subreddit = 'EtherMining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95643b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f'Loop {i}')\n",
    "    # If file does not exists, start pulling posts from current datetime\n",
    "    # else pull from file last post created_utc\n",
    "    if not path.isfile(file):\n",
    "        df=pd.DataFrame()\n",
    "        params = {\n",
    "            'subreddit': subreddit,\n",
    "            'size': 100,\n",
    "            'before': 1658758060\n",
    "        }\n",
    "    else:\n",
    "        df = pd.read_csv(file)\n",
    "        params = {\n",
    "            'subreddit': subreddit,\n",
    "            'size': 100,\n",
    "            'before': df.loc[df.shape[0]-1,'created_utc']\n",
    "    }\n",
    "         \n",
    "    success = False\n",
    "    \n",
    "    while not success:\n",
    "        try:\n",
    "            res = requests.get(url, params)\n",
    "            status = res.status_code\n",
    "            print(f'Get Status: {status}')\n",
    "            if status == 200:\n",
    "                success = True\n",
    "            else:\n",
    "                time.sleep(10)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            continue\n",
    "    \n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    temp_df = pd.DataFrame(posts)\n",
    "    pd.concat([df, temp_df]).to_csv(file, index=False)\n",
    "        \n",
    "    time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
